{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porosity analyis of the coloured pieces\n",
    "\n",
    "The initial state of this notebook is a copy of [the preview notebook](PreviewScans.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are used to set up the whole notebook.\n",
    "They load needed libraries and set some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pathlib\n",
    "import skimage\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "# This is loaded as a submodule to alleviate excessive copy-pasting between *all* projects we do\n",
    "# See https://github.com/habi/BrukerSkyScanLogfileRuminator for details on its inner workings\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code linting\n",
    "# %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "# We use the fast internal SSD for speed reasons\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_context(context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "scalefactor = 1\n",
    "plt.rcParams['figure.figsize'] = (16 // scalefactor, 9 // scalefactor)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the (tomographic) data can reside on different drives we set a folder to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        BasePath = os.path.join('N:\\\\')\n",
    "Root = os.path.join(BasePath, 'Schmid BFH Methylcellulose', 'RainbowRoll')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up, actually start to load/ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files present on disk\n",
    "# Using os.walk is way faster than using recursive glob.glob\n",
    "# Not sorting the found logfiles is also making it quicker\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]\n",
    "Data['FolderShort'] = [folder[len(Root) + 1:] for folder in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a (small) sampler of the loaded data as a first check\n",
    "Data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if 'TScopy' not in row.Folder and 'PR' not in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us\n",
    "            if not glob.glob(row.Folder.replace('proj', '*rec*')):\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for .csv files in each folder.\n",
    "# These are only generated when the \"X/Y Alignment With a Reference Scan\" was performed in NRecon.\n",
    "# If those files do *not* exist we have missed to do it and should correct for this.\n",
    "Data['XYAlignment'] = [glob.glob(os.path.join(f, '*T*.csv')) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display samples which are missing the .csv-files for the XY-alignment\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row['Folder']:\n",
    "        if not row['XYAlignment']:\n",
    "            if not any(x in row.LogFile for x in ['rectmp.log',  # because we only exclude temporary logfiles in a later step\n",
    "                                                  'proj_nofilter',  # since these two scans of single teeth don't contain a reference scan\n",
    "                                                  'TScopy',  # discard *t*hermal *s*hift data\n",
    "                                                  ]):\n",
    "                print('- %s has *not* been X/Y aligned' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the logfiles from all the folders that might be on disk but that we don't want to load the data from\n",
    "for c, row in Data.iterrows():\n",
    "    if 'proj' in os.path.split(row.Folder)[-1]:  # drop all projections folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif os.path.split(row.Folder)[-1] == 'TScopy':  # drop all phase retrieval folders for the moment\n",
    "        Data.drop([c], inplace=True)   \n",
    "    elif os.path.split(row.Folder)[-1] == 'PR':  # drop all phase retrieval folders for the moment\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:  # drop temporary log files of samples currently being reconstructed\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums in the dataframe\n",
    "Data['Sample'] = [pathlib.Path(log).parts[-3] for log in Data['LogFile']]\n",
    "Data['Scan'] = [os.path.basename(os.path.dirname(log)) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quickly show the data from the last loaded scans\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file names of all the reconstructions of all the scans\n",
    "Data['Filenames Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "# How many reconstructions do we have?\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data['Filenames Reconstructions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "# Reset the dataframe count/index for easier indexing afterwards\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Camera'] = [camera(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the parameters we extracted from the log files (with [our log file parser](https://github.com/habi/BrukerSkyScanLogfileRuminator)) to check for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample',\n",
    "      'Voxelsize', 'Camera', 'Filter', 'Averaging', 'RotationStep',\n",
    "      'RingartefactCorrection', 'BeamHardeningCorrection', 'DefectPixelMasking', 'Grayvalue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample',\n",
    "      'Voxelsize', 'Camera', 'Filter', 'Averaging', 'RotationStep',\n",
    "      'RingartefactCorrection', 'BeamHardeningCorrection', 'DefectPixelMasking', 'Grayvalue']].to_excel('Data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now that we've double-checked some of the parameters (and corrected any issues that might have shown up) we start to load the preview images.\n",
    "If the three cells below are uncommented, the machine-generated previews are shown, otherwise we just continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data['Filename PreviewImage'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp')))[0] for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [dask_image.imread.imread(pip).squeeze()\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['Filename PreviewImage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an approximately square overview image\n",
    "lines = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "    plt.imshow(row.PreviewImage.squeeze())\n",
    "    plt.title(row.Sample)\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                  'um',\n",
    "                                  color='black',\n",
    "                                  frameon=True))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Root, 'ScanOverviews.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we 'load' all reconstructions from disks into stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load all reconstructions DASK arrays\n",
    "# Reconstructions = [dask_image.imread.imread(os.path.join(folder,'*rec*.png')) for folder in Data['Folder']]\n",
    "# Load all reconstructions into ephemereal DASK arrays, with a nice progress bar...\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Loading reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression setting for .zarr files\n",
    "import numcodecs\n",
    "compressor = numcodecs.Zstd(level=3)  # level 1â€“22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in Data['FolderShort']:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out reconstructions to .zarr files for easy loading and handling\n",
    "# The resulting files can be loaded with https://github.com/saalfeldlab/n5-ij in ImageJ/Fiji\n",
    "# Once https://github.com/saalfeldlab/n5-ij/issues/106 is fixed, we can udpate the format version :)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving reconstructions to Zarr',\n",
    "                   total=len(Data)):\n",
    "    zfn = os.path.join(os.path.dirname(row.Folder),\n",
    "                       '%s.zarr') % (row.Sample)\n",
    "    Data.at[c, 'ZarrFileName'] = zfn\n",
    "    if not os.path.exists(zfn):\n",
    "        Reconstructions[c][:,:,:,0].rechunk(chunks=(128,128,128)).to_zarr(zfn,\n",
    "                                                                          compressor=compressor,\n",
    "                                                                          zarr_format=2)\n",
    "        print('Saved %s' % zfn[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reconstructions from the zarr files\n",
    "Reconstructions = [dask.array.from_zarr(file) for file in Data['ZarrFileName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we have on disk?\n",
    "print('We have %s reconstructions on %s' % (Data['Number of reconstructions'].sum(), Root))\n",
    "print('This is about %s reconstructions per scan (%s scans in %s folders)' % (round(Data['Number of reconstructions'].sum() / len(Data)),\n",
    "                                                                              len(Data),\n",
    "                                                                              len(Data.Sample.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Frontal',\n",
    "              'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the middle slices, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['Mid_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Central images',\n",
    "                   total=len(Data),\n",
    "                   leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'],\n",
    "                                             row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Center.%s.png' % (row['Sample'],\n",
    "                                                            row['Scan'],\n",
    "                                                            direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate requested axial view\n",
    "            if 'Axial' in direction:\n",
    "                mid = Reconstructions[c][Data['Size'][c][0] // 2]\n",
    "            if 'Frontal' in direction:\n",
    "                mid = Reconstructions[c][:, Data['Size'][c][1] // 2, :]\n",
    "            if 'Median' in direction:\n",
    "                mid = Reconstructions[c][:, :, Data['Size'][c][2] // 2]\n",
    "            # Write the calculated 'direction' view to disk\n",
    "            imageio.imwrite(outfilepath, mid)\n",
    "        Data.at[c, 'Mid_' + direction] = dask_image.imread.imread(outfilepath).squeeze()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show/save middle slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of central images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.CentralSlices.png' % (row['Sample']))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(skimage.exposure.equalize_adapthist(row['Mid_' + direction]).squeeze())\n",
    "            if d == 0:\n",
    "                plt.axhline(row.Size[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.axvline(row.Size[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[2]))\n",
    "            elif d == 1:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[2] // 2, c=seaborn.color_palette()[1])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[0]))\n",
    "            else:\n",
    "                plt.axhline(row.Size[0] // 2, c=seaborn.color_palette()[2])\n",
    "                plt.axvline(row.Size[1] // 2, c=seaborn.color_palette()[0])\n",
    "                plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                              'um',\n",
    "                                              color=seaborn.color_palette()[1]))\n",
    "            plt.title('%s\\nCentral %s slice' % (row['Sample'], direction.lower()))\n",
    "            plt.axis('off')\n",
    "        plt.savefig(outfilepath,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = [None] * len(Reconstructions)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Working on MIPs',\n",
    "                   total=len(Data),\n",
    "                   leave=False):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'],\n",
    "                                             row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'],\n",
    "                                                         row['Scan'],\n",
    "                                                         direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate MIP and write it to disk\n",
    "            mip = Reconstructions[c].max(axis=d)\n",
    "            imageio.imwrite(outfilepath, mip)\n",
    "        Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show/save MIP slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.MIPs.png' % (row['Sample']))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(skimage.exposure.equalize_adapthist(row['MIP_' + direction]).squeeze())\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s MIP' % direction)\n",
    "            plt.axis('off')\n",
    "            plt.title('%s\\n%s MIP' % (row['Sample'], direction))\n",
    "            plt.savefig(outfilepath,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further checking the data, we look at the gray value histogram of the reconstructions.\n",
    "This helps us to find scans that have not been reconstructed well and might either need to be repeated or simply re-reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of all the (subsampled) images\n",
    "subsample = 3\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Data['Histogram'] = [dask.array.histogram(rec[::subsample, ::subsample, ::subsample],\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for rec in Reconstructions]\n",
    "# We thus use row.Histogram[0] to plot the value below, doing the calculation only when necessary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define us a custom color palette, with the names of the samples\n",
    "ourcolorpallette = [sn for sn in Data['Sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all histograms\n",
    "for c, row in Data.iterrows():\n",
    "    if subsample == 1:\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.Histogram.png' % (row['Sample']))\n",
    "    else:\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.Histogram.Subsampled%02dx.png' % (row['Sample'],\n",
    "                                                                         subsample))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        plt.semilogy(row.Histogram[0], color=row.Sample)\n",
    "        plt.xlim([0, 2**8])\n",
    "        plt.ylim(ymin=10**0)\n",
    "        seaborn.despine()\n",
    "        if subsample != 1:\n",
    "            plt.title('%s\\nHistogram of %s x subsampled dataset' % (row['Sample'], subsample))\n",
    "        else:\n",
    "            plt.title('%s Histogram' % row['Sample'])\n",
    "        if not os.path.exists(outfilepath):    \n",
    "            plt.savefig(outfilepath,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all histograms\n",
    "for c, row in Data.iterrows():\n",
    "    plt.semilogy(row.Histogram[0], color=row.Sample, label=row['Sample'])\n",
    "plt.legend()\n",
    "plt.xlim([0, 2**8])\n",
    "plt.ylim(ymin=10**0)\n",
    "seaborn.despine()\n",
    "if subsample != 1:\n",
    "    plt.title('Histograms of %s x subsampled datasets' % subsample)\n",
    "else:\n",
    "    plt.title('All Histogram')\n",
    "if subsample == 1:\n",
    "    outfilepath = os.path.join(Root, 'Histograms.png')\n",
    "else:\n",
    "    outfilepath = os.path.join(Root, 'Histograms.Subsampled%02dx.png' % (subsample))\n",
    "print('Saving to %s' % outfilepath)\n",
    "plt.savefig(outfilepath,\n",
    "            bbox_inches='tight')            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have previewed %s scans of %s folders in %s' % (len(Data),\n",
    "                                                          len(Data.Sample.unique()),\n",
    "                                                          Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porosity(image, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the open porosity of a given image\n",
    "    - Calculate the largest contour, to define the outline\n",
    "    - Make a mask of this\n",
    "    - Threshold the image inside the mask\n",
    "    - Return the porosity as the ratio of 'air' pixels to 'total' pixels inside the mask\n",
    "    \"\"\"\n",
    "    # Calculate largest contour\n",
    "    contours = skimage.measure.find_contours(image, level=skimage.filters.threshold_otsu(image))\n",
    "    largest_contour = max(contours, key=lambda x: x.shape[0])\n",
    "    # Make a mask of this: https://stackoverflow.com/a/60888683\n",
    "    mask = numpy.zeros_like(image)\n",
    "    rr, cc = skimage.draw.polygon(largest_contour[:, 0], largest_contour[:, 1], mask.shape)\n",
    "    mask[rr, cc] = 1\n",
    "    # Threshold the image inside the mask\n",
    "    threshold = skimage.filters.threshold_otsu(image)\n",
    "    material = numpy.sum((image >= threshold) & (mask == 1))\n",
    "    # Calculate porosity\n",
    "    porosity = 1 - (material / numpy.sum(mask))\n",
    "    if verbose:\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original image')\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(image)\n",
    "        plt.plot(largest_contour[:, 1], largest_contour[:, 0], linewidth=2, color='orange')\n",
    "        plt.imshow(numpy.ma.masked_equal(mask, numpy.ma.masked_equal(image > threshold, 1)),\n",
    "                   alpha=0.5,\n",
    "                   cmap=\"viridis_r\")\n",
    "        plt.title('Porosity: %.2f%% (Outline: %spx, Air in outline: %s px, Threshold: %s)' % (porosity * 100,\n",
    "                                                                                              numpy.sum(mask),\n",
    "                                                                                              numpy.sum((image < threshold) & (mask == 1)),\n",
    "                                                                                              threshold))\n",
    "        plt.show()\n",
    "    return porosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Porosity'] = None\n",
    "stride = 5 # to speed up calculations, we only calculate every 33rd slice\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Calculating porosities on every %s-th slice' % stride,\n",
    "                   total=len(Data)):\n",
    "    Data.at[c, 'Porosity'] = [porosity(r.compute(), verbose=False) for r in tqdm(Reconstructions[c][::stride], desc=row.Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(values, std=3):\n",
    "    # Drop outliers based on their zscore: https://stackoverflow.com/a/23202269/323100\n",
    "    arr = numpy.array(values, dtype=float)\n",
    "    z = numpy.abs(scipy.stats.zscore(arr))\n",
    "    return arr[z < std].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from porosity data\n",
    "Data['Porosity_NoOutliers'] = [drop_outliers(p) for p in Data['Porosity']]\n",
    "Data['Porosity_NoOutliers_Mean'] = [numpy.mean(p)*100 for p in Data['Porosity_NoOutliers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Data['Porosity'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Data['Porosity_NoOutliers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean porosity\n",
    "Data['Porosity_Mean'] = [numpy.mean(p)*100 for p in Data['Porosity']]\n",
    "Data['Porosity_NoOutliers_Mean'] = [numpy.mean(p)*100 for p in Data['Porosity_NoOutliers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Porosity_Mean', 'Porosity_NoOutliers_Mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by the no-outliers-mean-porosity\n",
    "Data.sort_values(by='Porosity_NoOutliers_Mean', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Porosity_Mean', 'Porosity_NoOutliers_Mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripplot of porosities, with hue of sample\n",
    "seaborn.swarmplot(data=Data.explode('Porosity'),\n",
    "                  x='Sample',\n",
    "                  y='Porosity',\n",
    "                  hue='Sample',\n",
    "                  # dodge=True,\n",
    "                  palette=Data['Sample'].values.tolist())\n",
    "plt.title('Porosities of Rainbow Rolls samples, calculated on every %s-th slice' % stride)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripplot of porosities, with hue of sample\n",
    "seaborn.boxenplot(data=Data.explode('Porosity_NoOutliers'),\n",
    "                  x='Sample',\n",
    "                  y='Porosity_NoOutliers',\n",
    "                  hue='Sample',\n",
    "                  showfliers=False,\n",
    "                  palette=Data['Sample'].values.tolist(),\n",
    "                  saturation=1)\n",
    "seaborn.stripplot(data=Data.explode('Porosity_NoOutliers'),\n",
    "                  x='Sample',\n",
    "                  y='Porosity_NoOutliers',\n",
    "                  hue='Sample',\n",
    "                  size=10,\n",
    "                  linewidth=1,\n",
    "                  edgecolor='white',\n",
    "                  palette=Data['Sample'].values.tolist())\n",
    "seaborn.despine()\n",
    "plt.title('NO OUTLIERS BASED ON ORIGINALS ZSCORE\\nPorosities of Rainbow Rolls samples, calculated on every %s-th slice' % stride)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
