{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa8ea78-aab3-47a9-8b4f-8a2197a83fee",
   "metadata": {},
   "source": [
    "# Porespy analysis of the 'framed' scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356db810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "import skimage\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm, trange\n",
    "import porespy as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5691a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "# We use the fast internal SSD for speed reasons\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f718f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set preferred seaborn theme\n",
    "seaborn.set_theme(style='ticks',\n",
    "                  context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84cdd7-4dbf-41ff-a90d-388333a1dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "scalefactor = 1\n",
    "plt.rcParams['figure.figsize'] = (16 // scalefactor, 9 // scalefactor)  # Size up figures a bit\n",
    "# plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb202e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e700207-63fb-4e32-bec8-49eefd98adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "# This is loaded as a submodule to alleviate excessive copy-pasting between *all* projects we do\n",
    "# See https://github.com/habi/BrukerSkyScanLogfileRuminator for details on its inner workings\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570c5c6-9c9a-420f-ab03-b98a4d773ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Win' in platform.system():\n",
    "    Root = 'F:/'\n",
    "else:\n",
    "    Root = '/media/habi/Fast_SSD'\n",
    "Path = os.path.join(Root, 'Schmid BFH Methylcellulose')\n",
    "print('Our base path is %s' % Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb623cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574de289-177e-4d0e-8874-d58cd78045bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99fba6-8fc8-4cc3-bab4-8c55d1d50bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files present on disk\n",
    "# Using os.walk is way faster than using recursive glob.glob\n",
    "# Not sorting the found logfiles is also making it quicker\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Path)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1fa41-b1a2-48cf-8e6b-4394ee677db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]\n",
    "Data['FolderShort'] = [folder[len(Root) + 1:] for folder in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e89523-3e51-4711-b1ae-fb81ca970812",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a36cd-90f4-4660-9b11-58c26a9a65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the logfiles from all the folders that might be on disk but that we don't want to load the data from\n",
    "for c, row in Data.iterrows():\n",
    "    if 'proj' in os.path.split(row.Folder)[-1]:  # drop all projections folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    if os.path.split(row.Folder)[-1] == 'PR':  # drop all phase retrieval folders for the moment\n",
    "        Data.drop([c], inplace=True)        \n",
    "    elif 'rectmp.log' in row.LogFile:  # drop temporary log files of samples currently being reconstructed\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca5421-3e81-43c3-90a4-cbec5026ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums in the dataframe\n",
    "Data['Sample'] = [('-').join([pathlib.Path(log).parts[-4], pathlib.Path(log).parts[-3]]) for log in Data['LogFile']]\n",
    "Data['Scan'] = [os.path.basename(os.path.dirname(log)) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Sample.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272fe4e-e679-4a57-8241-1c4b71599c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file names of all the reconstructions of all the scans\n",
    "Data['Filenames Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "# How many reconstructions do we have?\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data['Filenames Reconstructions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b590aa-1dc4-4be7-8ccb-ee4736c409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "# Reset the dataframe count/index for easier indexing afterwards\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa1f4c-f818-4589-8ad8-e4bb9b3f3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Camera'] = [camera(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692822f-41e8-425c-96df-e4114be2535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the scans except 'Framed'\n",
    "for c, row in Data.iterrows():\n",
    "    if 'Blobs' in row.LogFile or 'Chunks' in row.LogFile or 'Kreidegrund' in row.LogFile:\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Sample', 'Scan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f750ea3-b6f2-472f-8eb5-975f8066e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Sample.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdaeb9e-1976-454f-84e6-34ede8907893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all reconstructions DASK arrays\n",
    "# Reconstructions = [dask_image.imread.imread(os.path.join(folder,'*rec*.png')) for folder in Data['Folder']]\n",
    "# Load all reconstructions into ephemereal DASK arrays, with a nice progress bar...\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Loading reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have more than one sample load only the selected one\n",
    "whichscan = 0\n",
    "print('Loading reconstructions for %s' % Data['Sample'][whichscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dcc32-4582-4e74-92b7-6008b8729fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[whichscan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622bf56-5379-4ce6-a374-3dd6813d4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image from the midle of the stack\n",
    "# Just the middle slice of the stack\n",
    "image = Reconstructions[whichscan][Reconstructions[whichscan].shape[0]//2].compute()\n",
    "# The middle slice in x\n",
    "# inputimage = Reconstructions[whichscan][:,Reconstructions[whichscan].shape[1]//2,:].compute()\n",
    "# The middle slice in \n",
    "# inputimage = Reconstructions[whichscan][:,:,Reconstructions[whichscan].shape[2]//2].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c17846-93ca-4152-9ab4-c75750c37c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show test image\n",
    "plt.imshow(skimage.exposure.equalize_adapthist(image))\n",
    "plt.title('Input image, %s x %s px' % (image.shape[0], image.shape[1]))\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and bottom of the image is empty, detect the region, based on its horizontal MIP\n",
    "grayvalue_vertical = numpy.max(image, axis=1)\n",
    "# Find region from left and right based on the Otsu threshold\n",
    "threshold_v = skimage.filters.threshold_otsu(grayvalue_vertical)\n",
    "buffer_v = 0\n",
    "top = numpy.where(grayvalue_vertical > threshold_v)[0][0] - buffer_v\n",
    "bottom = numpy.where(grayvalue_vertical > threshold_v)[0][-1] + buffer_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left and right of the image might be cut, too\n",
    "grayvalue_horizontal = numpy.max(image, axis=0)\n",
    "threshold_h = skimage.filters.threshold_otsu(grayvalue_horizontal)\n",
    "buffer_h = 0\n",
    "left = numpy.where(grayvalue_horizontal > threshold_h)[0][0] - buffer_h\n",
    "right = numpy.where(grayvalue_horizontal > threshold_h)[0][-1] + buffer_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e93401",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grayvalue_horizontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top/Bottom\n",
    "plt.subplot(121)\n",
    "plt.plot(grayvalue_vertical, range(len(grayvalue_vertical)))\n",
    "plt.axvline(threshold_v)\n",
    "plt.axhline(top, color='red', linestyle='--', label='top@%s' % top)\n",
    "plt.axhline(top + buffer_v, color='red', linestyle='--', alpha=0.5, label='top-buffer@%s' % str(top + buffer_v))\n",
    "plt.axhline(bottom - buffer_v, color='blue', linestyle='--', alpha=0.5, label='bottom-buffer@%s' % str(bottom- buffer_v))\n",
    "plt.axhline(bottom, color='blue', linestyle='--', label='bottom@%s' % bottom)\n",
    "plt.xlim(0, 255)\n",
    "plt.legend()\n",
    "plt.title('Vertical gray value MIP')\n",
    "seaborn.despine()\n",
    "plt.subplot(122)\n",
    "plt.plot(grayvalue_horizontal)\n",
    "plt.axhline(threshold_h)\n",
    "plt.axvline(left, color='red', linestyle='--', label='left@%s' % left)\n",
    "plt.axvline(left + buffer_h, color='red', linestyle='--', alpha=0.5, label='left+buffer@%s' % str(left+buffer_h))\n",
    "plt.axvline(right - buffer_h, color='blue', linestyle='--', alpha=0.5, label='right-buffer@%s' % str(right-buffer_h))\n",
    "plt.axvline(right, color='blue', linestyle='--', label='right@%s' % right)\n",
    "plt.ylim(0, 255)\n",
    "plt.legend()\n",
    "plt.title('Horizontal gray value sum')\n",
    "seaborn.despine()\n",
    "plt.suptitle(os.path.basename(Data['Filenames Reconstructions'][whichscan][len(Data['Filenames Reconstructions'][whichscan])//2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972b263-c77f-4114-bf2d-1ce30d26d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop top/bottom (calculated as left/right above)\n",
    "plt.subplot(211)\n",
    "plt.imshow(image)\n",
    "plt.plot(grayvalue_vertical, range(len(grayvalue_vertical)), label='Vertical gray value profile')\n",
    "plt.axhline(top, ls='dashed', c='w', alpha=0.618, label='Cut top@%s' % top)\n",
    "plt.axhline(bottom, ls='dashed', c='w', alpha=0.618, label='Cut bottom@%s' % bottom)\n",
    "plt.plot(grayvalue_horizontal)\n",
    "plt.axvline(left, ls='dotted', c='w', alpha=0.618, label='Cut left@%s' % left)\n",
    "plt.axvline(right, ls='dotted', c='w', alpha=0.618, label='Cut right@%s' % right)\n",
    "plt.legend()\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.axis('off')\n",
    "plt.subplot(212)\n",
    "croppedimage = image[top:bottom, left:right]\n",
    "plt.axis('off')\n",
    "plt.imshow(skimage.exposure.equalize_adapthist(croppedimage))\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41814779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, buffer_h=1, buffer_v=1, verbose=False):\n",
    "    '''Crop input image to smallest left:right,top:bottom rectangle, based on horizontal and vertical MIP'''\n",
    "    # Top and bottom of the image is empty, detect the region, based on its horizontal MIP\n",
    "    grayvalue_vertical = numpy.max(image, axis=1)\n",
    "    threshold_v = skimage.filters.threshold_otsu(grayvalue_vertical)\n",
    "    top = numpy.where(grayvalue_vertical > threshold_v)[0][0] - buffer_v\n",
    "    bottom = numpy.where(grayvalue_vertical > threshold_v)[0][-1] + buffer_v\n",
    "    # Left and right of the image might be cut, too\n",
    "    grayvalue_horizontal = numpy.max(image, axis=0)\n",
    "    threshold_h = skimage.filters.threshold_otsu(grayvalue_horizontal)\n",
    "    left = numpy.where(grayvalue_horizontal > threshold_h)[0][0] - buffer_h\n",
    "    right = numpy.where(grayvalue_horizontal > threshold_h)[0][-1] + buffer_h\n",
    "    croppedimage = image[top:bottom, left:right]\n",
    "    if verbose:\n",
    "        # Top/Bottom\n",
    "        plt.subplot(221)\n",
    "        plt.plot(grayvalue_vertical, range(len(grayvalue_vertical)))\n",
    "        plt.axvline(threshold_v)\n",
    "        plt.axhline(top, color='red', linestyle='--', label='top@%s' % top)\n",
    "        plt.axhline(top + buffer_v, color='red', linestyle='--', alpha=0.5, label='top-buffer@%s' % str(top + buffer_v))\n",
    "        plt.axhline(bottom - buffer_v, color='blue', linestyle='--', alpha=0.5, label='bottom-buffer@%s' % str(bottom- buffer_v))\n",
    "        plt.axhline(bottom, color='blue', linestyle='--', label='bottom@%s' % bottom)\n",
    "        plt.xlim(0, 255)\n",
    "        plt.legend()\n",
    "        plt.title('Vertical gray value MIP')\n",
    "        seaborn.despine()\n",
    "        plt.subplot(222)\n",
    "        plt.plot(grayvalue_horizontal)\n",
    "        plt.axhline(threshold_h)\n",
    "        plt.axvline(left, color='red', linestyle='--', label='left@%s' % left)\n",
    "        plt.axvline(left + buffer_h, color='red', linestyle='--', alpha=0.5, label='left+buffer@%s' % str(left+buffer_h))\n",
    "        plt.axvline(right - buffer_h, color='blue', linestyle='--', alpha=0.5, label='right-buffer@%s' % str(right-buffer_h))\n",
    "        plt.axvline(right, color='blue', linestyle='--', label='right@%s' % right)\n",
    "        plt.ylim(0, 255)\n",
    "        plt.legend()\n",
    "        plt.title('Horizontal gray value sum')\n",
    "        seaborn.despine()\n",
    "        plt.subplot(223)\n",
    "        plt.imshow(image)\n",
    "        plt.title('%s x %s px' % (image.shape[0], image.shape[1]))\n",
    "        plt.plot(grayvalue_vertical, range(len(grayvalue_vertical)), label='Vertical gray value profile')\n",
    "        plt.axhline(top, ls='dashed', c='w', alpha=0.618, label='Cut top@%s' % top)\n",
    "        plt.axhline(bottom, ls='dashed', c='w', alpha=0.618, label='Cut bottom@%s' % bottom)\n",
    "        plt.plot(grayvalue_horizontal)\n",
    "        plt.axvline(left, ls='dotted', c='w', alpha=0.618, label='Cut left@%s' % left)\n",
    "        plt.axvline(right, ls='dotted', c='w', alpha=0.618, label='Cut right@%s' % right)\n",
    "        plt.legend()\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(224)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(skimage.exposure.equalize_adapthist(croppedimage))\n",
    "        plt.title('%s x %s px' % (croppedimage.shape[0], croppedimage.shape[1]))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.show()\n",
    "    return(croppedimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_iso = skimage.filters.threshold_isodata(croppedimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc564c-f458-45a6-98b7-cd5c8e890bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizedimage = croppedimage < threshold_iso  # porespy expects 'True' for features of interest, so we true stuff smaller than the threshold, e.g the air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2357076-ef02-409c-913c-c18d32584461",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.imshow(croppedimage)\n",
    "plt.title('Center of original slice, %s x %s px' % (croppedimage.shape[0], croppedimage.shape[1]))\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.axis('off')\n",
    "plt.subplot(212)\n",
    "plt.imshow(~binarizedimage) # Invert for displaying\n",
    "plt.title('Binarized image, %s x %s px' % (binarizedimage.shape[0], binarizedimage.shape[1]))\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = ps.filters.local_thickness(binarizedimage)\n",
    "pt = ps.filters.porosimetry(binarizedimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5272a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(lt)\n",
    "plt.subplot(122)\n",
    "plt.imshow(pt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b075a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize difference\n",
    "plt.imshow(lt-pt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21d4f0",
   "metadata": {},
   "source": [
    "Local thickness according to https://porespy.org/examples/filters/reference/local_thickness.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7c851-ec7c-47db-9c9f-3c1b7e97d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate local thickness\n",
    "localthickness = ps.filters.local_thickness(binarizedimage, sizes=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844a028-d20c-4c11-a255-dc0b4bd69b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show local thickness over binarized image\n",
    "plt.imshow(localthickness/binarizedimage, cmap='viridis')\n",
    "plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "plt.axis('off')\n",
    "plt.title('Local thickness overlaid over original')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01251f-be18-4950-860a-5997f1516a32",
   "metadata": {},
   "source": [
    "Pore size distribution as per https://porespy.org/examples/metrics/reference/pore_size_distribution.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8d5fd-0e50-433e-86f7-a342a929f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = ps.metrics.pore_size_distribution(localthickness,\n",
    "                                        bins=100,\n",
    "                                        log=False,\n",
    "                                        voxel_size=Data['Voxelsize'][whichscan])\n",
    "print(psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db414f54-c27e-45db-8b7a-ac1e331995d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probability density function, based on https://porespy.org/examples/metrics/reference/pore_size_distribution.html\n",
    "plt.bar(x=psd.R, height=psd.pdf, width=psd.bin_widths)\n",
    "plt.xlabel('Pore radius [um]')\n",
    "plt.ylabel('Probability density function')\n",
    "plt.xlim([0, 500])\n",
    "seaborn.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de05387",
   "metadata": {},
   "source": [
    "Now that we have \"singled out\" all the different steps, we can put them together into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f31a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all together into a \"function\" that we can call for each reconstruction\n",
    "def analyze_reconstruction(whichscan, whichreconstruction, verbose=True):\n",
    "    # Load the requested reconstruction\n",
    "    inputimage = Reconstructions[whichscan][whichreconstruction].compute()\n",
    "    if verbose:\n",
    "        print('Analyzing reconstruction %s of sample %s (%s)' % (whichreconstruction,\n",
    "                                                                 Data['Sample'][whichscan],\n",
    "                                                                 Data['Filenames Reconstructions'][whichscan][whichreconstruction][len(Root) + 1:]))\n",
    "        plt.imshow(inputimage)\n",
    "        plt.title('%s, %s x %s px @ %0.2f um' % (Data['Filenames Reconstructions'][whichscan][whichreconstruction][len(Root) + 1:],\n",
    "                                                 inputimage.shape[0], inputimage.shape[1],\n",
    "                                                 Data['Voxelsize'][whichscan]))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    # Crop it\n",
    "    croppedimage = crop_image(inputimage, verbose=verbose)\n",
    "    # Threshold with isodata, which - according to ChatGPT - is best for gradual transitions and less clearly defined histogram modes.\n",
    "    threshold = skimage.filters.threshold_isodata(croppedimage)\n",
    "    if verbose:\n",
    "        # Display gray value histogram of image\n",
    "        histogram = plt.hist(croppedimage.ravel(),\n",
    "                            bins='doane',\n",
    "                            histtype='step',\n",
    "                            log=True,\n",
    "                            label='Histogram',\n",
    "                            color=seaborn.color_palette()[0])\n",
    "        plt.axvline(threshold, label='Threshold@%s' % threshold, c=seaborn.color_palette()[1])\n",
    "        plt.xlim([0, 255])\n",
    "        plt.legend()\n",
    "        plt.title('Logarithmic grayvalue histogram with %s bins' % len(histogram[1]))\n",
    "        seaborn.despine()\n",
    "        plt.show()\n",
    "    binarizedimage = croppedimage < threshold  # porespy expects 'True' for features of interest, so we true stuff smaller than the threshold, e.g the air\n",
    "    if verbose:\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(croppedimage)\n",
    "        plt.title('Original, cropped, %s x %s px' % (croppedimage.shape[0],\n",
    "                                                     croppedimage.shape[1]))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(~binarizedimage) # Invert for display\n",
    "        plt.title('Original, cropped and binarized @ %s, %s x %s px' % (threshold,\n",
    "                                                                        binarizedimage.shape[0],\n",
    "                                                                        binarizedimage.shape[1]))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    # Calculate local thickness according to https://porespy.org/examples/filters/reference/local_thickness.html\n",
    "    localthickness = ps.filters.local_thickness(binarizedimage,\n",
    "                                                sizes=100, \n",
    "                                                )\n",
    "    if verbose:\n",
    "        # Show local thickness over binarized image\n",
    "        plt.imshow(localthickness/binarizedimage, cmap='viridis')\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan],'um'))\n",
    "        plt.axis('off')\n",
    "        plt.title('Local thickness overlaid over binarized image')\n",
    "        plt.show()\n",
    "    # Calculate pore size distribution, as per https://porespy.org/examples/metrics/reference/pore_size_distribution.html\n",
    "    psd = ps.metrics.pore_size_distribution(localthickness,\n",
    "                                            bins=50,\n",
    "                                            log=False,\n",
    "                                            voxel_size=Data['Voxelsize'][whichscan])\n",
    "    if verbose:\n",
    "        print(psd)\n",
    "    # Visualize results\n",
    "    # Plot probability density function, based on https://porespy.org/examples/metrics/reference/pore_size_distribution.html\n",
    "    plt.figure()\n",
    "    plt.bar(x=psd.R, height=psd.pdf, width=psd.bin_widths)\n",
    "    plt.xlabel('Pore radius [um]')\n",
    "    plt.ylabel('Probability density function')\n",
    "    plt.title('Pore size distribution of %s' % (Data['Filenames Reconstructions'][whichscan][whichreconstruction][len(Root) + 1:]))\n",
    "    plt.xlim([0, 700])\n",
    "    plt.ylim([0, 0.015])\n",
    "    seaborn.despine()    \n",
    "    os.makedirs(os.path.join(os.path.dirname(Data['Folder'][whichscan]), 'psd'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(os.path.dirname(Data['Folder'][whichscan]), 'psd', 'psd_%s' % os.path.basename(Data['Filenames Reconstructions'][whichscan][whichreconstruction])))\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = ps.metrics.pore_size_distribution(localthickness,\n",
    "#                                             bins=2**8,\n",
    "#                                             log=False,\n",
    "#                                             voxel_size=Data['Voxelsize'][whichscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(results['R'], results['pdf'], label='PDF', color='navy')\n",
    "# # plt.plot(results['R'], results['cdf'], label='CDF', color='darkorange')\n",
    "# plt.xlabel('Pore Radius (units of voxel size)')\n",
    "# plt.ylabel('Frequency / Cumulative')\n",
    "# plt.title('Pore Size Distribution')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_reconstruction(0, 1001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e07172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample N reconstructions\n",
    "for i in tqdm(numpy.random.sample(777)):\n",
    "    analyze_reconstruction(0,\n",
    "                           int(i*len(Data['Filenames Reconstructions'][whichscan])),\n",
    "                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichscan = 0\n",
    "# for i in trange(len(Data['Filenames Reconstructions'][whichscan]),\n",
    "#                    desc='Analyzing reconstructions',\n",
    "#                    total=len(Data['Filenames Reconstructions'][whichscan])):\n",
    "#     if not i % 222:\n",
    "#         analyze_reconstruction(whichscan, i, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd40d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
